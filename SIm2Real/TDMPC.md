### Abstract
- Data driven MPC has advantages:
	- could improve sample efficiency 
	- better performance with increased computational budget
- We combine the strengths of model and model-free
	- learned task-oriented latent dynamics model for local trajectory optimization over short horizon
	- learned terminal value function to estimate long-term return
- As a result, TD-MPC achieves superior sample efficiency and asymptotic performance on both state and image-based continuous control tasks

*Latent Dynamics Model*
- Latent space is a lower dimensional space where the essential features of the original high-dimensional data are preserved.

### Intro
- Model-based learning Is expensive to plan over long horizons
- Model-based methods have historically struggled to outperform simpler model free methods in continuous control tasks.
- MPC optimizes a trajectory over a short, finite horizon due to immense cost of long-horizon planning.
- MPC can be extended to approximate globally optimal solutions by using a terminal value function that estimates discounted return beyond the planning horizon
- However, obtaining an accurate model and value function can be challenging

- TD-MPC is data-driven MPC using a task-oriented latent dynamics model and terminal value function learned jointly by temporal difference learning.
- At each decision step, we perform trajectory optimization using short-term reward estimates generated by the learned model.
- For example, Humanoid locomotion task panning with a model may be beneficial for accurate joint movement, whereas the higher-level object such as direction can be guided by long-term value estimates

- The key technical contribution is how the model is learned. 
- Prior works learns a model through state or video prediction, but this is inefficient
- To overcome challenges, we make 3 key changes
	- learn the latent representation of the dynamics model purely from reward to make learning 
	- back-propagate gradients from the reward and TD-objective through multiple rollout s


*Terminal Value Function*
- estimate long-term return from a future stte at the end of horizon
